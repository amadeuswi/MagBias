{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# reproduce the yang zhang paper (1104.2487v2) on weighting for magnification bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need a lot of averages for this, all with respect to magnitude, in particular we need:\n",
    "\n",
    "$\\alpha-1$\n",
    "$b_g$ and\n",
    "$W$\n",
    "\n",
    "$W$ itself requires the same averages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "#$#$#$#$#$#$#$#$#$#$#$#$#$#$#$#$#$#$#$#$#$#$#$#$#$#$#$#$#$#$ \n",
      "Rescaling the galaxy number density by a factor of 104185.986576 to match the gold sample with 6330073646.61 total galaxies \n",
      "#$#$#$#$#$#$#$#$#$#$#$#$#$#$#$#$#$#$#$#$#$#$#$#$#$#$#$#$#$#$ \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# from magmod import shotnoise, C_l_DM_CAMB, bgal_new, sg\n",
    "from magmod import *\n",
    "from magbias_experiments import SKA_zhangpen, CLAR_zhangpen, SKA1, SKA2, cb_hirax as hirax, hirax512, LSST, LSST_nosgfit, n\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "bgal_new has the same magnitude dependence as sg, ONLY ON THE BIN CENTER\n",
    "\n",
    "shotnoise for the magnitude bin is straight forward\n",
    "\n",
    "C_DM does not depend on magnitude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Ngal_in_bin(zmin, mtab_edges, galsurv, NINT = 2000, maxmag = 27):\n",
    "    \"\"\"mtab_edges is magnitude bin edges, \n",
    "    upper means higher number and fainter, thus more galaxies\"\"\"\n",
    "\n",
    "    zmax = zbg_max(maxmag, galsurv)\n",
    "    z_integrate = np.linspace(zmin,zmax, NINT)\n",
    "\n",
    "\n",
    "    dNdztab = np.array([nofz(z_integrate, mmm) for mmm in mtab_edges])\n",
    "    dNdzdOm = np.trapz(dNdztab, z_integrate, axis = 1)\n",
    "    \n",
    "\n",
    "    \n",
    "    #now subtract lower from upper\n",
    "    Nz = (dNdzdOm[1:] - dNdzdOm[:-1]) * 4 * np.pi #already in rad!!!! and multiplying with all sky\n",
    "    Nz = np.atleast_1d(Nz)\n",
    "    if (Nz<0).any():\n",
    "#         print Nz\n",
    "        raise ValueError(\"cannot have negative number of galaxies, most likely we confused magnitudes here\")\n",
    "    return Nz\n",
    "\n",
    "\n",
    "\n",
    "# def Ngal_in_bin_old(zmin, zmax, m_lower, m_upper, galsurv, NINT = 2000):\n",
    "#     \"\"\"m_lower and m_upper ar the lower and upper magnitude bin edges, \n",
    "#     upper means higher number and fainter, thus more galaxies\"\"\"\n",
    "\n",
    "\n",
    "#     z_integrate = np.linspace(zmin,zmax, NINT)\n",
    "\n",
    "\n",
    "#     dNdztab_lower = nofz(z_integrate, m_lower)\n",
    "#     dNdztab_upper = nofz(z_integrate, m_upper)\n",
    "    \n",
    "    \n",
    "#     dNzdOm_lower = np.trapz(dNdztab_lower, z_integrate)\n",
    "#     dNzdOm_upper = np.trapz(dNdztab_upper, z_integrate)\n",
    "    \n",
    "# #     print dNzdOm_upper - dNzdOm_lower\n",
    "\n",
    "    \n",
    "#     #now subtract lower from upper\n",
    "#     Nz = (dNzdOm_upper - dNzdOm_lower) * 4 * np.pi #already in rad!!!! and multiplying with all sky\n",
    "#     Nz = np.atleast_1d(Nz)\n",
    "#     if (Nz<0).any():\n",
    "# #         print Nz\n",
    "#         raise ValueError(\"cannot have negative number of galaxies, most likely we confused magnitudes here\")\n",
    "#     return Nz\n",
    "\n",
    "def shotnoise_in_bin(zmin, mtab_edges, galsurv, maxmag = 27, NINT = 2000):\n",
    "    return 4*pi / Ngal_in_bin(zmin, mtab_edges, galsurv, NINT, maxmag = maxmag)\n",
    "\n",
    "# def shotnoise_in_bin(zmin, zmax, m_lower, m_upper, galsurv, NINT = 2000):\n",
    "#     \"\"\"m_lower and m_upper ar the lower and upper magnitude bin edges, \n",
    "#     upper means higher number and fainter, thus more galaxies\"\"\"\n",
    "\n",
    "\n",
    "#     z_integrate = np.linspace(zmin,zmax, NINT)\n",
    "\n",
    "\n",
    "#     dNdztab_lower = nofz(z_integrate, m_lower)\n",
    "#     dNdztab_upper = nofz(z_integrate, m_upper)\n",
    "    \n",
    "    \n",
    "#     dNzdOm_lower = np.trapz(dNdztab_lower, z_integrate)\n",
    "#     dNzdOm_upper = np.trapz(dNdztab_upper, z_integrate)\n",
    "    \n",
    "# #     print dNzdOm_upper - dNzdOm_lower\n",
    "\n",
    "    \n",
    "#     #now subtract lower from upper\n",
    "#     Nz = (dNzdOm_upper - dNzdOm_lower) * 4 * np.pi #already in rad!!!! and multiplying with all sky\n",
    "#     Nz = np.atleast_1d(Nz)\n",
    "#     if (Nz<0).any():\n",
    "# #         print Nz\n",
    "#         raise ValueError(\"cannot have negative number of galaxies, most likely we confused magnitudes here\")\n",
    "#     return 4*pi/Nz #shot noise! The 4 pi could be cancelled but this way I can check more easily that it's correct...\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def alphaminusone(z, maxmag, exp = LSST): #2(alpha-1) = 5sg-2\n",
    "    maxmag = np.atleast_1d(maxmag)\n",
    "    res = np.array( [ 5*sg(z, exp, mmm)[0] - 2 for mmm in maxmag])\n",
    "    return res\n",
    "\n",
    "def average_no_W(A,B,N):\n",
    "    \"\"\"A,B are the arrays to be averaged, same length as N which is the bg number density of galaxies\"\"\"\n",
    "    return np.sum(A*B*N, axis = 1)/np.sum(N) #A and B are matrices, ell x mag\n",
    "\n",
    "\n",
    "\n",
    "def W_weight(C_DMtab, Cshottab, alpha_m_one_tab, bgal_tab, Ngal_tab, exp = LSST):\n",
    "    \"\"\"the optimal, scale dependent weight, eq. 10\n",
    "    input: background redshift z, magnitude bin center mag (later will be full array, now just needs to be inside the table used for bgal_tab),\n",
    "    array (for ell values) of dark matter power spectrum and the shot noise (number). \n",
    "    Then there are tables of alpha-1, bgal, Cshot and Ngal on a given magnitude binning (the binning itself not needed here)\"\"\"\n",
    "\n",
    "    DM_by_shot = np.outer(C_DMtab,1/Cshottab) #this need to have right shape, as C_DM is length ell and Cshot is length magtab\n",
    "    num = -average_no_W(alpha_m_one_tab, bgal_tab, Ngal_tab) * DM_by_shot\n",
    "    denom = 1 + average_no_W(bgal_tab,bgal_tab, Ngal_tab) * DM_by_shot\n",
    "    fac1 = num/denom\n",
    "\n",
    "    \n",
    "#     return np.outer( np.ones( len(C_DMtab)),alpha_m_one_tab) + bgal_tab * fac1 #outer product to get right shape\n",
    "    return alpha_m_one_tab + bgal_tab * fac1 #outer product to get right shape\n",
    "\n",
    "\n",
    "\n",
    "def W_weight_singlebin(z, mag, C_DMtab, Cshottt, alpha_m_one_tab, bgal_tab, Ngal_tab, exp = LSST):\n",
    "    \"\"\"the optimal, scale dependent weight, eq. 10\n",
    "    input: background redshift z, magnitude bin center mag (later will be full array, now just needs to be inside the table used for bgal_tab),\n",
    "    array (for ell values) of dark matter power spectrum and the shot noise (number). \n",
    "    Then there are tables of alpha-1, bgal, Cshot and Ngal on a given magnitude binning (the binning itself not needed here)\"\"\"\n",
    "\n",
    "    DM_by_shot = C_DMtab/Cshottt #this need to have right shape, as C_DM is length ell and Cshot is length magtab\n",
    "    \n",
    "    num = -average_no_W(alpha_m_one_tab, bgal_tab, Ngal_tab) * DM_by_shot\n",
    "    denom = 1 + average_no_W(bgal_tab,bgal_tab, Ngal_tab) * DM_by_shot\n",
    "    fac1 = num/denom\n",
    "    bgalll = bgal_new(z, mag, experiment = exp)\n",
    "    almino = alphaminusone(z, mag, exp = exp)\n",
    "    \n",
    "    \n",
    "    print bgalll, \"bg\"\n",
    "    print almino, \"alpha minus one\"\n",
    "    print Cshottt, \"Cshot\"\n",
    "    \n",
    "    return almino + fac1 * bgalll\n",
    "    \n",
    "    \n",
    "\n",
    "# def average_W(A,B,N,weights):\n",
    "#     \"\"\"average weighted by weights, important: weights will be matrix (ell x mags), thus A and be also need same shape, \n",
    "#     and N need length mags.\"\"\"\n",
    "#     W_ar = W_weight(mags)\n",
    "#     return average_no_W(A,B,N*weights)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "testing things:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: lminSKA is set to large value 200!\n"
     ]
    }
   ],
   "source": [
    "# B1 1st bin:\n",
    "zflow = 0.34127101; zfhigh = 0.84127101; zfmean = (zfhigh+zflow)/2; delta_zf = (zfhigh-zflow)/2\n",
    "\n",
    "#background z:\n",
    "buffer_z = 0.1\n",
    "zlow = zfhigh + buffer_z\n",
    "#zhigh should not be needed\n",
    "\n",
    "#small mag range:\n",
    "magmin = 25; magmax = 27; Nmag = 3\n",
    "magtab_edges = np.linspace(magmin, magmax, Nmag)\n",
    "magtab_centers = (magtab_edges[1:] + magtab_edges[:-1])/2\n",
    "\n",
    "mag_test = magtab_centers[1]\n",
    "magmin_test = magtab_edges[1]; magmax_test = magtab_edges[2]\n",
    "\n",
    "\n",
    "#small ell range\n",
    "SKAarea = SKA['S_area']\n",
    "# lminSKA = np.amax([10,np.int(np.around(2*pi/np.sqrt(SKAarea)))]) #never use lmin <20\n",
    "lminSKA = 200\n",
    "print \"WARNING: lminSKA is set to large value {}!\".format(lminSKA)\n",
    "lend = 2200;\n",
    "ltabSKA = np.arange(lminSKA, lend + lminSKA, lminSKA)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "C_DM_tab = C_l_DM_CAMB(ltabSKA, zlow, galsurv = LSST)\n",
    "Cshot_test = shotnoise_in_bin(zlow, magtab_edges, LSST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "alphaminusone_tab = alphaminusone(zlow, magtab_centers) #actually a mean redshift should be used\n",
    "bias_g_tab = np.array([bgal_new(zlow, mmm) for mmm in magtab_centers])\n",
    "N_g_tab = Ngal_in_bin(zlow,magtab_edges, LSST)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is unclear to me what mean redshift to use for the background galaxies. Because here alpha-1 and bg are not integrated over, but evaluated at a single redshift..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "Wtest = W_weight(C_DM_tab, Cshot_test, alphaminusone_tab, bias_g_tab, N_g_tab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.19355644 -0.16345818]\n",
      " [ 0.12752405 -0.20259644]\n",
      " [ 0.05578155 -0.25022057]\n",
      " [-0.01560434 -0.30390536]\n",
      " [-0.0749769  -0.35429278]\n",
      " [-0.12394613 -0.40049931]\n",
      " [-0.16415348 -0.44208213]\n",
      " [-0.19592445 -0.47757745]\n",
      " [-0.22173716 -0.50832285]\n",
      " [-0.24269249 -0.53465606]\n",
      " [-0.25985189 -0.55720605]]\n"
     ]
    }
   ],
   "source": [
    "print Wtest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assuming that the weighting function works, we also need the interferometer and dish noise, the foreground power spectrum and C_HI-mu. For the last one we use eq. 6 to relate to Cl_HIxmag.\n",
    "\n",
    "Do not forget: galaxy background, HI foreground"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 11)\n"
     ]
    }
   ],
   "source": [
    "C_HIHI_tab = C_l_HIHI_CAMB(ltabSKA, zflow, zfhigh)\n",
    "C_noise_fg_tab = noise_cls_single_dish(ltabSKA, ztonu21(zfmean), SKA1, 256) * np.ones( len(ltabSKA) )\n",
    "C_HIxmag_tab = np.array( [Cl_HIxmag_CAMB(ltabSKA, zfmean, delta_zf, zlow, MAXMAG = mmm)\n",
    "                          for mmm in magtab_centers]).T #transpose to match shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def S2N_weighted(ltab, deltaell, fsky, Cfg, CSfg, CSbg, CHImu, biasg, Weight, Ngal):\n",
    "    num = (2*ltab + 1) * deltaell * fsky\n",
    "    denom = 1 + (Cfg + CSfg) * ( \n",
    "        average_no_W(biasg, Weight, Ngal)**2 * C_DM_tab + \n",
    "        average_no_W(Weight**2, CSbg, Ngal)) / (average_no_W(CHImu, Weight, Ngal)**2) #I put the shot noise into the average\n",
    "#         average_no_W(Weight, Weight, Ngal) * CSbg) / (average_no_W(CHImu, Weight, Ngal)**2)\n",
    "    frac = num/denom\n",
    "    res = np.sum(frac, axis = 0)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "delta ell = 200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/local/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/ipykernel_launcher.py:5: RuntimeWarning: overflow encountered in true_divide\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "deltaell = ltabSKA[1]-ltabSKA[0]\n",
    "print \"delta ell = {}\".format(deltaell)\n",
    "fsky = SKA1[\"S_area\"] / (4*np.pi)\n",
    "\n",
    "S2Ntest = S2N_weighted(ltabSKA, deltaell, fsky, C_HIHI_tab, C_noise_fg_tab, Cshot_test, C_HIxmag_tab, bias_g_tab, Wtest, N_g_tab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17.586356750476753\n"
     ]
    }
   ],
   "source": [
    "print S2Ntest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this S2N is about a factor of 2 better than in our paper (fig 7 dark blue). But a few open questions remain to this approach:\n",
    "\n",
    "-- we should put bg and alpha into the integrations, i.e. the power spectra into the averaging\n",
    "\n",
    "-- shot noise NEEDS to be inside the averaging, typo/error in their paper???\n",
    "\n",
    "-- I am honestly unsure what weighting with a non-positive, sign-flipping weight actually means, but I guess in our case it just corrects for negative signals (demag)?\n",
    "\n",
    "-- we must keep in mind that we fixed (alpha - 1)bg ourselves, and this has a big impact here too.\n",
    "\n",
    "-- I should check with smaller ∆ell!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
